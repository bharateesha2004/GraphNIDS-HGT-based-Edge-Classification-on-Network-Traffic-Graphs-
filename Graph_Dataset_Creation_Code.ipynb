{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading, Sampling, and Splitting (UNSW-NB15)\n",
    "\n",
    "This code block performs the initial data preparation steps for the UNSW-NB15 dataset. It involves loading the data, defining appropriate column names, sampling a manageable subset, and splitting this subset into training, validation, and testing sets while maintaining the original class distribution.\n",
    "\n",
    "1.  **Import Libraries:**\n",
    "    * `pandas` is imported for data manipulation and reading CSV files.\n",
    "    * `train_test_split` from `sklearn.model_selection` is used for splitting the data strategically.\n",
    "\n",
    "2.  **Define Column Names:**\n",
    "    * A list `column_names` is created containing all 49 feature names for the UNSW-NB15 dataset, including the target labels (`attack_cat`, `Label`).\n",
    "\n",
    "3.  **Load Dataset:**\n",
    "    * The dataset is loaded from `/kaggle/input/1-dataset/UNSW-NB15_1.csv` using `pd.read_csv`.\n",
    "    * `header=None` is specified because the original CSV file does not contain a header row.\n",
    "    * `names=column_names` assigns the predefined column names to the DataFrame.\n",
    "    * The total number of rows (expected: 700,000) and columns (expected: 49) are printed for verification.\n",
    "\n",
    "4.  **Stratified Sampling:**\n",
    "    * A smaller sample of 240,000 rows is drawn from the full dataset.\n",
    "    * `train_test_split` is used for this sampling step (even though it's typically for train/test splitting) by specifying `train_size=240000`.\n",
    "    * `stratify=df['Label']` ensures that the proportion of normal (`Label=0`) and attack (`Label=1`) instances in the 240,000-row sample is the same as in the original 700,000-row dataset. This is crucial for representative sampling.\n",
    "    * `random_state=42` ensures reproducibility of the sampling process.\n",
    "\n",
    "5.  **Train/Validation/Test Split:**\n",
    "    * The 240,000-row sample (`sampled_df`) is split into:\n",
    "        * A combined training and validation set (`train_val_df`) of 200,000 rows.\n",
    "        * A test set (`test_df`) of 40,000 rows.\n",
    "        * This split is also stratified by `Label` using `stratify=sampled_df['Label']` and uses `random_state=42`.\n",
    "    * The 200,000-row `train_val_df` is further split into:\n",
    "        * A training set (`train_df`) of 160,000 rows.\n",
    "        * A validation set (`val_df`) of 40,000 rows.\n",
    "        * This split is again stratified by `Label` using `stratify=train_val_df['Label']` and uses `random_state=42`.\n",
    "\n",
    "6.  **Verification:**\n",
    "    * The shapes (number of rows) of the final `train_df`, `val_df`, and `test_df` are printed to confirm the sizes (160k, 40k, 40k respectively).\n",
    "    * The normalized distribution (`value_counts(normalize=True)`) of the `Label` column is printed for each of the three sets (train, validation, test) to verify that stratification has successfully maintained similar class proportions across all splits.\n",
    "\n",
    "7.  **Save Splits:**\n",
    "    * The resulting `train_df`, `val_df`, and `test_df` DataFrames are saved as CSV files (`train_set.csv`, `val_set.csv`, `test_set.csv`) in the `/kaggle/working/` directory.\n",
    "    * `index=False` prevents pandas from writing the DataFrame index as a column in the CSV files.\n",
    "\n",
    "This process results in three distinct, stratified datasets ready for model training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-09T17:16:31.034680Z",
     "iopub.status.busy": "2025-04-09T17:16:31.034211Z",
     "iopub.status.idle": "2025-04-09T17:16:44.698737Z",
     "shell.execute_reply": "2025-04-09T17:16:44.697653Z",
     "shell.execute_reply.started": "2025-04-09T17:16:31.034626Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-7e091f760f36>:16: DtypeWarning: Columns (1,3,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('/kaggle/input/1-dataset/UNSW-NB15_1.csv', names=column_names, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 700001\n",
      "Columns: 49\n",
      "Column names: ['srcip', 'sport', 'dstip', 'dsport', 'proto', 'state', 'dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss', 'dloss', 'service', 'Sload', 'Dload', 'Spkts', 'Dpkts', 'swin', 'dwin', 'stcpb', 'dtcpb', 'smeansz', 'dmeansz', 'trans_depth', 'res_bdy_len', 'Sjit', 'Djit', 'Stime', 'Ltime', 'Sintpkt', 'Dintpkt', 'tcprtt', 'synack', 'ackdat', 'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd', 'is_ftp_login', 'ct_ftp_cmd', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'attack_cat', 'Label']\n",
      "Sampled rows: 240000\n",
      "Train+Val rows: 200000, Test rows: 40000\n",
      "Training rows: 160000, Validation rows: 40000\n",
      "Training label distribution:\n",
      " Label\n",
      "0    0.968263\n",
      "1    0.031738\n",
      "Name: proportion, dtype: float64\n",
      "Validation label distribution:\n",
      " Label\n",
      "0    0.96825\n",
      "1    0.03175\n",
      "Name: proportion, dtype: float64\n",
      "Testing label distribution:\n",
      " Label\n",
      "0    0.968275\n",
      "1    0.031725\n",
      "Name: proportion, dtype: float64\n",
      "Datasets saved: train_set.csv, val_set.csv, test_set.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the column names\n",
    "column_names = [\n",
    "    'srcip', 'sport', 'dstip', 'dsport', 'proto', 'state', 'dur', 'sbytes', 'dbytes', \n",
    "    'sttl', 'dttl', 'sloss', 'dloss', 'service', 'Sload', 'Dload', 'Spkts', 'Dpkts', \n",
    "    'swin', 'dwin', 'stcpb', 'dtcpb', 'smeansz', 'dmeansz', 'trans_depth', 'res_bdy_len', \n",
    "    'Sjit', 'Djit', 'Stime', 'Ltime', 'Sintpkt', 'Dintpkt', 'tcprtt', 'synack', 'ackdat', \n",
    "    'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd', 'is_ftp_login', 'ct_ftp_cmd', \n",
    "    'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm', 'ct_src_dport_ltm', \n",
    "    'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'attack_cat', 'Label'\n",
    "]\n",
    "\n",
    "# Load the dataset with column names\n",
    "df = pd.read_csv('/kaggle/input/1-dataset/UNSW-NB15_1.csv', names=column_names, header=None)\n",
    "print(f\"Total rows: {df.shape[0]}\")  # Should be 700,000\n",
    "print(f\"Columns: {df.shape[1]}\")     # Should be 49\n",
    "print(\"Column names:\", df.columns.tolist())\n",
    "\n",
    "# Step 1: Sample 240,000 rows with stratification based on 'Label'\n",
    "sampled_df, _ = train_test_split(df, train_size=240000, stratify=df['Label'], random_state=42)\n",
    "print(f\"Sampled rows: {sampled_df.shape[0]}\")  # Should be 240,000\n",
    "\n",
    "# Step 2: Split into train+val (200,000) and test (40,000)\n",
    "train_val_df, test_df = train_test_split(sampled_df, train_size=200000, stratify=sampled_df['Label'], random_state=42)\n",
    "print(f\"Train+Val rows: {train_val_df.shape[0]}, Test rows: {test_df.shape[0]}\")\n",
    "\n",
    "# Step 3: Split train+val into train (160,000) and val (40,000)\n",
    "train_df, val_df = train_test_split(train_val_df, train_size=160000, stratify=train_val_df['Label'], random_state=42)\n",
    "print(f\"Training rows: {train_df.shape[0]}, Validation rows: {val_df.shape[0]}\")\n",
    "\n",
    "# Verify label distribution\n",
    "print(\"Training label distribution:\\n\", train_df['Label'].value_counts(normalize=True))\n",
    "print(\"Validation label distribution:\\n\", val_df['Label'].value_counts(normalize=True))\n",
    "print(\"Testing label distribution:\\n\", test_df['Label'].value_counts(normalize=True))\n",
    "\n",
    "# Save the splits to CSV files\n",
    "train_df.to_csv('/kaggle/working/train_set.csv', index=False)\n",
    "val_df.to_csv('/kaggle/working/val_set.csv', index=False)\n",
    "test_df.to_csv('/kaggle/working/test_set.csv', index=False)\n",
    "print(\"Datasets saved: train_set.csv, val_set.csv, test_set.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7097501,
     "sourceId": 11343800,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
